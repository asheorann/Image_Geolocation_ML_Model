# -*- coding: utf-8 -*-
"""CIS_5190_Applied_ML_Image_Geolocation_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1evqhATmAGmpeU39qZfPlNR7Phb_04U46

# Installing dependencies

Please make a copy of this notebook.
"""

!pip install geopy > delete.txt
!pip install datasets > delete.txt
!pip install torch torchvision datasets > delete.txt
!pip install huggingface_hub > delete.txt
!rm delete.txt

# ----------------------
# Core Python / Utilities
# ----------------------
import os
import copy

# ----------------------
# Numerical / Scientific
# ----------------------
import numpy as np

# ----------------------
# PyTorch
# ----------------------
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import StepLR

# ----------------------
# TorchVision
# ----------------------
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.models import resnet50, ResNet50_Weights

# ----------------------
# Hugging Face
# ----------------------
from transformers import AutoImageProcessor, AutoModelForImageClassification
from huggingface_hub import PyTorchModelHubMixin
from datasets import load_dataset, Image

# ----------------------
# Evaluation / Metrics
# ----------------------
from sklearn.metrics import mean_absolute_error, mean_squared_error

# ----------------------
# Visualization
# ----------------------
import matplotlib.pyplot as plt

# ----------------------
# Image & Geospatial
# ----------------------
from PIL import Image
from geopy.distance import geodesic

# ----------------------
# For ConvNextTiny and KNN
# ----------------------
from tqdm import tqdm  # <--- This was missing!
from sklearn.neighbors import NearestNeighbors
from torchvision.models import ConvNeXt_Tiny_Weights
import numpy as np

"""# Huggingface login
You will require your personal token.

"""

!huggingface-cli login

"""# Data

## Downloading the train and test dataset
"""

from datasets import load_dataset, Image

dataset_train = load_dataset("AnoushkaMenon/CIS519_Image2GPS", split="train")
dataset_test = load_dataset("AnoushkaMenon/CIS519_Image2GPS", split="test")
dataset_val   = load_dataset("AnoushkaMenon/CIS519_Image2GPS", split="validation")

import matplotlib.pyplot as plt
import numpy as np

# Extract GPS coordinates from training set
train_lats = np.array(dataset_train["Latitude"])
train_lons = np.array(dataset_train["Longitude"])

plt.figure(figsize=(6, 6))
plt.scatter(train_lons, train_lats, s=10, alpha=0.6)

plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Training GPS Locations")
plt.grid(True)

plt.show()

print(dataset_train)
print(dataset_test)
print(dataset_val)

"""## Defining the Custom Dataset Class"""

class CustomConvNeXt(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        # Load pre-trained ConvNeXt Tiny
        self.backbone = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)

        # Save the input features dimension (usually 768 for Tiny)
        n_inputs = self.backbone.classifier[2].in_features

        # Replace the head with your regressor
        self.backbone.classifier[2] = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(n_inputs, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

    # Helper to extract features for k-NN
    def get_features(self, x):
        x = self.backbone.features(x)
        x = self.backbone.avgpool(x)
        x = self.backbone.classifier[0](x) # LayerNorm
        x = self.backbone.classifier[1](x) # Flatten
        return x

class GPSImageDataset(Dataset):
    def __init__(self, hf_dataset, transform=None, lat_mean=None, lat_std=None, lon_mean=None, lon_std=None):
        self.hf_dataset = hf_dataset
        self.transform = transform

        # Compute mean and std from the dataframe if not provided
        self.latitude_mean = lat_mean if lat_mean is not None else np.mean(np.array(self.hf_dataset['Latitude']))
        self.latitude_std = lat_std if lat_std is not None else np.std(np.array(self.hf_dataset['Latitude']))
        self.longitude_mean = lon_mean if lon_mean is not None else np.mean(np.array(self.hf_dataset['Longitude']))
        self.longitude_std = lon_std if lon_std is not None else np.std(np.array(self.hf_dataset['Longitude']))

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        # Extract data
        example = self.hf_dataset[idx]

        # Load and process the image
        image = example['image']
        latitude = example['Latitude']
        longitude = example['Longitude']
        # image = image.rotate(-90, expand=True)
        if self.transform:
            image = self.transform(image)

        # Normalize GPS coordinates
        latitude = (latitude - self.latitude_mean) / self.latitude_std
        longitude = (longitude - self.longitude_mean) / self.longitude_std
        gps_coords = torch.tensor([latitude, longitude], dtype=torch.float32)

        return image, gps_coords

"""## Creating dataloaders and visualizing the data"""

# -----------------------
# Transforms
# -----------------------

# Training transform (NO harmful augmentations)
# Training transform (ONLY color jitter)
transform = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),  # small zoom & aspect jitter
    transforms.RandomHorizontalFlip(p=0.5),                                # flip for robustness (optional)
    transforms.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.15,
        hue=0.04
    ),
    transforms.RandomApply([
        transforms.GaussianBlur(kernel_size=3)
    ], p=0.1),  # slight blur occasionally

    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])


# Validation / inference transform (same preprocessing, no randomness)
inference_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# -----------------------
# Train dataset / loader
# -----------------------
train_dataset = GPSImageDataset(
    hf_dataset=dataset_train,
    transform=transform
)
train_dataloader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True
)

# Extract normalization stats FROM TRAIN ONLY (no leakage)
lat_mean, lat_std = train_dataset.latitude_mean, train_dataset.latitude_std
lon_mean, lon_std = train_dataset.longitude_mean, train_dataset.longitude_std

# -----------------------
# Validation dataset / loader
# -----------------------
val_dataset = GPSImageDataset(
    hf_dataset=dataset_val,          # <-- validation split
    transform=inference_transform,
    lat_mean=lat_mean, lat_std=lat_std,
    lon_mean=lon_mean, lon_std=lon_std
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False
)

# -----------------------
# Test dataset / loader
# -----------------------
test_dataset = GPSImageDataset(
    hf_dataset=dataset_test,         # <-- test split
    transform=inference_transform,
    lat_mean=lat_mean, lat_std=lat_std,
    lon_mean=lon_mean, lon_std=lon_std
)
test_dataloader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False
)

print("Dataloaders ready:",
      f"train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}")

# Verify loading
for images, gps_coords in train_dataloader:
    print(images.size(), gps_coords.size())
    break

def denormalize(tensor, mean, std):
    mean = np.array(mean)
    std = np.array(std)
    tensor = tensor.numpy().transpose((1, 2, 0))  # Convert from C x H x W to H x W x C
    tensor = std * tensor + mean  # Denormalize
    tensor = np.clip(tensor, 0, 1)  # Clip to keep pixel values between 0 and 1
    return tensor

data_iter = iter(train_dataloader)
images, gps_coords = next(data_iter)  # Get a batch of images and labels
# Denormalize the first image in the batch for display
itr = 0
for im in images:
  image = denormalize(im, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

  # Plot the image
  plt.imshow(image)
  plt.title(f'Latitude: {gps_coords[itr][0].item():.4f}, Longitude: {gps_coords[itr][1].item():.4f}')
  plt.axis('off')
  plt.show()
  itr += 1

"""# ConvNeXt-Tiny"""

TRAIN_MODE = "baseline"     # "baseline" or "freeze_unfreeze"

# -----------------------
# Build model (always the same start)
# -----------------------
model = CustomConvNeXt(num_classes=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model = model.to(device) # We will use 'model' instead of 'resnet' now
#resnet.fc = nn.Linear(resnet.fc.in_features, 2)
# resnet.fc = nn.Sequential(
#     nn.Dropout(p=0.3),  # Dropout with 30% rate
#     nn.Linear(resnet.fc.in_features, 2)
# )

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")
# resnet = resnet.to(device)

criterion = nn.MSELoss()

def rmse_meters(model, dataloader, lat_mean, lat_std, lon_mean, lon_std):
    model.eval()
    total_sq, total_n = 0.0, 0
    with torch.no_grad():
        for images, gps_coords in dataloader:
            images = images.to(device)
            gps_coords = gps_coords.to(device)

            outputs = model(images)

            preds_denorm = outputs.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])
            actuals_denorm = gps_coords.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])

            for pred, actual in zip(preds_denorm, actuals_denorm):
                d = geodesic((actual[0], actual[1]), (pred[0], pred[1])).meters
                total_sq += d * d
                total_n += 1
    return float(np.sqrt(total_sq / max(total_n, 1)))

def extract_all_features(model, dataloader, device):
    """
    Extracts feature vectors and corresponding GPS coordinates from the dataset.
    """
    model.eval()
    all_features = []
    all_coords = []

    print("Extracting features for k-NN...")
    with torch.no_grad():
        for images, gps_coords in tqdm(dataloader):
            images = images.to(device)
            # Use the custom feature extraction method
            features = model.get_features(images)

            all_features.append(features.cpu().numpy())
            all_coords.append(gps_coords.cpu().numpy())

    all_features = np.concatenate(all_features, axis=0)
    all_coords = np.concatenate(all_coords, axis=0)

    return all_features, all_coords

def knn_search_and_predict(test_features, train_features, train_coords, k=5):
    """
    Builds the k-NN index on training data and uses it to predict
    GPS coordinates for test features by averaging the k nearest neighbors.
    """
    print(f"Building k-NN index with k={k}...")
    # 1. Build the k-NN model (using L2/Euclidean distance by default)
    # We train the search structure on the extracted training features
    knn = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='euclidean')
    knn.fit(train_features)

    print("Finding nearest neighbors and predicting coordinates...")

    # 2. Find the k nearest neighbors for all test images
    # distances: distance to each neighbor, indices: index of each neighbor in train_coords
    distances, indices = knn.kneighbors(test_features)

    predicted_coords = []

    # 3. Predict the coordinate for each test image
    for i in tqdm(range(len(test_features))):
        # Get the normalized GPS coordinates of the k nearest neighbors
        neighbor_coords = train_coords[indices[i]]

        # Calculate the prediction by taking the simple average of the neighbors' coordinates
        # You could also implement weighted averaging using the distances, but simple average is a good start.
        avg_coord = np.mean(neighbor_coords, axis=0)
        predicted_coords.append(avg_coord)

    return np.array(predicted_coords)


# -----------------------
# TRAINING MODES
# -----------------------
best_val_rmse = float("inf")
best_state = None

if TRAIN_MODE == "baseline":
    print("\n=== TRAIN_MODE: baseline (fine-tune full model) ===")

    # Train all parameters from the start
    for p in model.parameters():
        p.requires_grad = True

    num_epochs = 12
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=5e-4,
        weight_decay=1e-4
    )
    scheduler = StepLR(optimizer, step_size=4, gamma=0.1) # og .1 gamma

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for images, gps_coords in train_dataloader:
            images, gps_coords = images.to(device), gps_coords.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, gps_coords)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        train_loss = running_loss / len(train_dataloader)
        val_rmse = rmse_meters(model, val_dataloader, lat_mean, lat_std, lon_mean, lon_std)

        print(f"Epoch [{epoch+1}/{num_epochs}]  TrainLoss: {train_loss:.4f}  ValRMSE(m): {val_rmse:.2f}")

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            best_state = copy.deepcopy(model.state_dict())

elif TRAIN_MODE == "freeze_unfreeze":
    print("\n=== TRAIN_MODE: freeze_unfreeze ===")

    # ===== Stage 1: freeze backbone, train head only =====
    for p in model.parameters():
        p.requires_grad = False

    head_layer=model.backbone.classifier[2]
    print("Unfreezing custom head at model.backbone.classifier[2]")
    for p in head_layer.parameters():
        p.requires_grad = True


    stage1_epochs = 4
    optimizer = optim.AdamW(head_layer.parameters(), lr=5e-4, weight_decay=1e-4)
    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)

    print("\n--- Stage 1: train fc only (backbone frozen) ---")
    for epoch in range(stage1_epochs):
        model.train()
        running_loss = 0.0

        for images, gps_coords in train_dataloader:
            images, gps_coords = images.to(device), gps_coords.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, gps_coords)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        train_loss = running_loss / len(train_dataloader)
        val_rmse = rmse_meters(model, val_dataloader, lat_mean, lat_std, lon_mean, lon_std)

        print(f"Stage1 Epoch [{epoch+1}/{stage1_epochs}]  TrainLoss: {train_loss:.4f}  ValRMSE(m): {val_rmse:.2f}")

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            best_state = copy.deepcopy(model.state_dict())

    # ===== Stage 2: unfreeze all, fine-tune gently =====
    for p in model.parameters():
        p.requires_grad = True

    stage2_epochs = 8
    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)

    print("\n--- Stage 2: fine-tune full model (unfrozen) ---")
    for epoch in range(stage2_epochs):
        model.train()
        running_loss = 0.0

        for images, gps_coords in train_dataloader:
            images, gps_coords = images.to(device), gps_coords.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, gps_coords)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        train_loss = running_loss / len(train_dataloader)
        val_rmse = rmse_meters(model, val_dataloader, lat_mean, lat_std, lon_mean, lon_std)

        print(f"Stage2 Epoch [{epoch+1}/{stage2_epochs}]  TrainLoss: {train_loss:.4f}  ValRMSE(m): {val_rmse:.2f}")

        if val_rmse < best_val_rmse:
            best_val_rmse = val_rmse
            best_state = copy.deepcopy(model.state_dict())

else:
    raise ValueError("TRAIN_MODE must be 'baseline' or 'freeze_unfreeze'")

print(f"\nBest Val RMSE (meters): {best_val_rmse:.2f}")
model.load_state_dict(best_state)

save_name = f"model_best_{TRAIN_MODE}.pth"
torch.save(model.state_dict(), save_name)
print("Saved:", save_name)

# ---------------------------------------------------------
# 2. RUN K-NN EVALUATION
#    (Assumes 'model', 'train_dataloader', 'val_dataloader',
#     and 'device' are still in memory from the previous cell)
# ---------------------------------------------------------
print("\n=== Starting k-NN Evaluation ===")

# --- A. Prepare Features ---
# Extract features from the training set (Your feature database)
train_features, train_coords_norm = extract_all_features(model, train_dataloader, device)

# Extract features from the validation set (The set you want to test the k-NN on)
val_features, val_coords_norm = extract_all_features(model, val_dataloader, device)

# --- B. Run k-NN Prediction ---
k_value = 15
predicted_val_coords_norm = knn_search_and_predict(
    test_features=val_features,
    train_features=train_features,
    train_coords=train_coords_norm,
    k=k_value
)

# --- C. Denormalize Predictions ---
lat_std_lon_std = np.array([lat_std, lon_std])
lat_mean_lon_mean = np.array([lat_mean, lon_mean])

predicted_val_coords = predicted_val_coords_norm * lat_std_lon_std + lat_mean_lon_mean
actual_val_coords = val_coords_norm * lat_std_lon_std + lat_mean_lon_mean

# --- D. Calculate k-NN RMSE (in meters) ---
total_sq, total_n = 0.0, 0
for pred, actual in zip(predicted_val_coords, actual_val_coords):
    d = geodesic((actual[0], actual[1]), (pred[0], pred[1])).meters
    total_sq += d * d
    total_n += 1

knn_rmse_meters = float(np.sqrt(total_sq / max(total_n, 1)))

print(f"\nâœ… k-NN (k={k_value}) Val RMSE (meters): {knn_rmse_meters:.2f}")

"""# Testing the model"""

# Load their toy dataset
# https://huggingface.co/datasets/gydou/released_img
dataset_ext = load_dataset("gydou/released_img", split="train")

# Build external dataset/dataloader using SAME normalization as training
ext_dataset = GPSImageDataset(
    hf_dataset=dataset_ext,
    transform=inference_transform,
    lat_mean=lat_mean, lat_std=lat_std,
    lon_mean=lon_mean, lon_std=lon_std
)
ext_dataloader = DataLoader(ext_dataset, batch_size=32, shuffle=False)

# Print out information to ensure everything looks good
print("External dataset size:", len(ext_dataset))
print("External columns:", dataset_ext.column_names)
print(f"Sample data: {dataset_ext[0]}")  # Show the first sample to ensure correct format

from sklearn.metrics import mean_absolute_error, mean_squared_error

def compare_methods(model, dataloader, train_features, train_coords_norm, dataset_name="External Data"):
    """
    Evaluates both Direct Regression AND k-NN on the same dataset.
    """
    model.eval()

    # --- METHOD A: Direct Regression (Standard) ---
    reg_preds = []
    reg_actuals = []
    reg_distances = []

    raw_preds = []
    raw_actuals = []

    # --- METHOD B: k-NN Preparation ---
    test_features = []

    print(f"Evaluating {dataset_name}...")

    with torch.no_grad():
        for images, gps_coords in tqdm(dataloader, desc="Running Inference"):
            images = images.to(device)
            gps_coords = gps_coords.to(device)

            # 1. Direct Regression Output
            outputs = model(images)

            # === FIX 2: Save the raw normalized data here ===
            raw_preds.append(outputs.cpu().numpy())
            raw_actuals.append(gps_coords.cpu().numpy())

            # 2. Extract Features for k-NN
            feats = model.get_features(images)
            test_features.append(feats.cpu().numpy())

            # Store Regression predictions (denormalized)
            p_batch = outputs.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])
            a_batch = gps_coords.cpu().numpy() * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])

            reg_preds.append(p_batch)
            reg_actuals.append(a_batch)

    # Process Regression Results
    reg_preds = np.vstack(reg_preds)
    reg_actuals = np.vstack(reg_actuals)

    for p, a in zip(reg_preds, reg_actuals):
        reg_distances.append(geodesic((a[0], a[1]), (p[0], p[1])).meters)

    # --- METHOD B: Run k-NN Search ---
    test_features = np.vstack(test_features)

    # Predict using k-NN (k=10 is usually a sweet spot)
    k_value = 15
    knn_preds_norm = knn_search_and_predict(test_features, train_features, train_coords_norm, k=k_value)

    # Denormalize k-NN predictions
    knn_preds = knn_preds_norm * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])

    knn_distances = []
    for p, a in zip(knn_preds, reg_actuals): # Use same actuals
        knn_distances.append(geodesic((a[0], a[1]), (p[0], p[1])).meters)

    # --- PRINT COMPARISON ---
    print(f"\n====== RESULTS: {dataset_name} ======")
    print(f"{'Metric':<20} | {'Direct Regression':<18} | {'k-NN (k=15)':<18}")
    print("-" * 65)
    print(f"{'Avg Error (meters)':<20} | {np.mean(reg_distances):<18.2f} | {np.mean(knn_distances):<18.2f}")
    print(f"{'Median Error (m)':<20} | {np.median(reg_distances):<18.2f} | {np.median(knn_distances):<18.2f}")
    print("-" * 65)
    all_preds = np.vstack(raw_preds)
    all_actuals = np.vstack(raw_actuals)

    # Return both for further analysis if needed
    return reg_distances, knn_distances, all_preds, all_actuals

# Run the comparison on your external dataset
reg_dists, knn_dists, all_preds, all_actuals = compare_methods(model, ext_dataloader, train_features, train_coords_norm, "Gydou Dataset")
reg_dists, knn_dists, all_preds, all_actuals= compare_methods(model, test_dataloader, train_features, train_coords_norm, "our test dataset")

"""#FOR PLOTTING ETC."""

# Retrieve normalization parameters from the training dataset
lat_mean = train_dataset.latitude_mean
lat_std = train_dataset.latitude_std
lon_mean = train_dataset.longitude_mean
lon_std = train_dataset.longitude_std

# Denormalize predictions and actual values
all_preds_denorm = all_preds * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])
all_actuals_denorm = all_actuals * np.array([lat_std, lon_std]) + np.array([lat_mean, lon_mean])

plt.figure(figsize=(10, 5))

# Plot actual points
plt.scatter(all_actuals_denorm[:, 1], all_actuals_denorm[:, 0], label='Actual', color='blue', alpha=0.6)

# Plot predicted points
plt.scatter(all_preds_denorm[:, 1], all_preds_denorm[:, 0], label='Predicted', color='red', alpha=0.6)

# Draw lines connecting actual and predicted points
for i in range(len(all_actuals_denorm)):
    plt.plot(
        [all_actuals_denorm[i, 1], all_preds_denorm[i, 1]],
        [all_actuals_denorm[i, 0], all_preds_denorm[i, 0]],
        color='gray', linewidth=0.5
    )

plt.legend()
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Actual vs. Predicted GPS Coordinates with Error Lines')
plt.show()

import torch
import copy
from tqdm import tqdm

# ==============================================================================
# 1. HELPER FUNCTION: EXTRACT FEATURES
# ==============================================================================
def extract_bank_features(model, dataloader, device):
    """
    Runs inference on the entire training set to build the k-NN bank.
    Returns:
        bank_feats: Tensor of shape (N, 768) - Normalized features
        bank_coords: Tensor of shape (N, 2)   - Normalized GPS coordinates
    """
    model.eval()
    feats_list = []
    coords_list = []

    print("Extracting features from training set for k-NN bank...")
    with torch.no_grad():
        for images, gps_coords in tqdm(dataloader, desc="Building Bank"):
            images = images.to(device)
            gps_coords = gps_coords.to(device)

            # Extract features using your model's specific method
            # Note: This calls model.get_features, NOT model.forward
            features = model.get_features(images)

            # Normalize features immediately (Critical for Cosine Similarity later)
            features = torch.nn.functional.normalize(features, p=2, dim=1)

            feats_list.append(features.cpu())
            coords_list.append(gps_coords.cpu())

    # Concatenate all batches into single large tensors
    bank_feats = torch.cat(feats_list, dim=0)
    bank_coords = torch.cat(coords_list, dim=0)

    return bank_feats, bank_coords

# ==============================================================================
# 2. MAIN BAKING & SAVING SCRIPT
# ==============================================================================

# Ensure model is in eval mode and on the correct device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

# --- A. Extract the k-NN Bank ---
bank_feats, bank_coords = extract_bank_features(model, train_dataloader, device)
print(f"Bank created. Features: {bank_feats.shape}, Coords: {bank_coords.shape}")

# --- B. Register Buffers (The "Baking" Process) ---
# We use a try/except block to handle cases where buffers might already exist
# if you re-run this cell multiple times.

# 1. k-NN Data
try:
    model.register_buffer("bank_feats", bank_feats)
    model.register_buffer("bank_coords", bank_coords)
except AttributeError:
    # If using DataParallel or similar wrappers, access .module
    if hasattr(model, 'module'):
        model.module.register_buffer("bank_feats", bank_feats)
        model.module.register_buffer("bank_coords", bank_coords)
    else:
        # If the buffer already exists (e.g. re-running cell), we can just update it
        model.bank_feats = bank_feats.to(model.bank_feats.device)
        model.bank_coords = bank_coords.to(model.bank_coords.device)

# 2. Normalization Statistics
# We grab these directly from your dataset object
# (Ensure train_dataset is the one you used for training)
lat_mean = torch.tensor(train_dataset.latitude_mean, dtype=torch.float32)
lat_std = torch.tensor(train_dataset.latitude_std, dtype=torch.float32)
lon_mean = torch.tensor(train_dataset.longitude_mean, dtype=torch.float32)
lon_std = torch.tensor(train_dataset.longitude_std, dtype=torch.float32)

model.register_buffer("lat_mean", lat_mean)
model.register_buffer("lat_std", lat_std)
model.register_buffer("lon_mean", lon_mean)
model.register_buffer("lon_std", lon_std)

print("Normalization stats registered.")

# --- C. Verification ---
# Let's double check everything is in the state_dict
state_dict = model.state_dict()
required_keys = ["bank_feats", "bank_coords", "lat_mean", "lat_std", "lon_mean", "lon_std"]
missing = [k for k in required_keys if k not in state_dict]

if missing:
    print(f"WARNING: The following keys are missing from state_dict: {missing}")
else:
    print("SUCCESS: All keys present in state_dict.")

# --- D. Save the Final Model ---
save_name = "model.pt"
torch.save(state_dict, save_name)

print(f"\nDONE! Saved '{save_name}' ({os.path.getsize(save_name)/1e6:.2f} MB).")
print("Download this file and submit it along with your model.py and preprocess.py.")

